Do transactions need an open_count field too?

We don't store older node versions as deltas yet.

Get rid of proplists; just give the user an APR hash table instead.

Produce helpful error messages when filename paths contain null
characters.

The delta algorithm walks the whole tree using a single pool, so the
memory used is proportional to the size of the target tree.  Instead,
it should use a separate subpool every time it recurses into a new
directory, and free that subpool as soon as it's done processing that
subdirectory, so the memory used is proportional to the depth of the
tree.

Probably file contents and delta contents should be stored in a
separate table from the REPRESENTATION and NODE-VERSION skel, so we
can parse those skels without having to read the whole file's data.
Perhaps a "strings" table, where a skel of the form ("strings" NUM)
instead of an atom means that the text should actually come from that
table.  This would also help commits work better --- we could create
the string objects while the user builds the transaction, and then
move the transaction nodes into the real node table without having to
touch all that text --- we just touch all that metainformation.

svn_fs__getsize shouldn't rely on a maximum value for detecting
overflow.

The use of svn_fs__getsize in svn_fs__parse_id is ugly --- what if
svn_vernum_t and apr_size_t aren't the same size?


Long term ideas:

- directory entry cache:
  Create a cache mapping a node version id X plus a filename component
  N onto a new node version id Y, meaning that X is a directory in
  which the name N is bound to ID Y.  If everything were in the cache,
  this function could run with no I/O except for the final node.

  Since node versions never change, we wouldn't have to worry about
  invalidating the cache.  Mutable node objects will need special
  handling, of course.

- fulltext cache:
  If we've recently computed a node's fulltext, we might want to keep
  that around in case we need to compute one of its nearby ancestors'
  fulltext, too.  This could be a waste, though --- best to record
  some data on how many delta applications the cache would avoid
  before implementing it.

- delta cache:
  As people update, we're going to be recomputing text deltas for the
  most recently changed files pretty often.  It might be worthwhile to
  cache the deltas for a little while.


Keeping repositories alive in the long term: Berkeley DB is infamous
for changing its file format from one version to the next.  If someone
saves a Subversion 1.0 repository on a CD somewhere, and then tries to
read it seven years later, their chance of being able to read it with
the latest version of Subversion is nil.  The solution:

- Define a simply XML repository dump format for the complete
  repository data.  This should be the same format we use for CVS
  repository conversion.  We'll have an import function.

- Write a program that is simple and self-contained --- does not use
  Berkeley DB, no fancy XML tools, uses nothing but POSIX read and
  seek --- that can dump a Subversion repository in that format.

- For each version of Subversion, make a sample repository, and
  archive a copy of it away as test data.

- Write a test suite that verifies that the repository dump program
  can handle all of the archived formats.
